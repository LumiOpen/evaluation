{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909281,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909281
    },
    "hendrycksTest-anatomy": {
      "acc": 0.42962962962962964,
      "acc_stderr": 0.04276349494376599,
      "acc_norm": 0.42962962962962964,
      "acc_norm_stderr": 0.04276349494376599
    },
    "hendrycksTest-astronomy": {
      "acc": 0.46710526315789475,
      "acc_stderr": 0.040601270352363966,
      "acc_norm": 0.46710526315789475,
      "acc_norm_stderr": 0.040601270352363966
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.49,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.49,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.47924528301886793,
      "acc_stderr": 0.03074634997572347,
      "acc_norm": 0.47924528301886793,
      "acc_norm_stderr": 0.03074634997572347
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.04166666666666666,
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.04166666666666666
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.41,
      "acc_stderr": 0.049431107042371025,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.049431107042371025
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.03794012674697029,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.03794012674697029
    },
    "hendrycksTest-college_physics": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.04440521906179326,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.04440521906179326
    },
    "hendrycksTest-computer_security": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.61,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3872340425531915,
      "acc_stderr": 0.03184389265339525,
      "acc_norm": 0.3872340425531915,
      "acc_norm_stderr": 0.03184389265339525
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2543859649122807,
      "acc_stderr": 0.040969851398436716,
      "acc_norm": 0.2543859649122807,
      "acc_norm_stderr": 0.040969851398436716
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.496551724137931,
      "acc_stderr": 0.041665675771015785,
      "acc_norm": 0.496551724137931,
      "acc_norm_stderr": 0.041665675771015785
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3201058201058201,
      "acc_stderr": 0.024026846392873502,
      "acc_norm": 0.3201058201058201,
      "acc_norm_stderr": 0.024026846392873502
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.040735243221471255,
      "acc_norm": 0.29365079365079366,
      "acc_norm_stderr": 0.040735243221471255
    },
    "hendrycksTest-global_facts": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5129032258064516,
      "acc_stderr": 0.02843453315268186,
      "acc_norm": 0.5129032258064516,
      "acc_norm_stderr": 0.02843453315268186
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2955665024630542,
      "acc_stderr": 0.032104944337514575,
      "acc_norm": 0.2955665024630542,
      "acc_norm_stderr": 0.032104944337514575
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620333
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.4909090909090909,
      "acc_stderr": 0.03903698647748441,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.03903698647748441
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5505050505050505,
      "acc_stderr": 0.0354413249194797,
      "acc_norm": 0.5505050505050505,
      "acc_norm_stderr": 0.0354413249194797
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6476683937823834,
      "acc_stderr": 0.03447478286414357,
      "acc_norm": 0.6476683937823834,
      "acc_norm_stderr": 0.03447478286414357
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.41025641025641024,
      "acc_stderr": 0.024939313906940774,
      "acc_norm": 0.41025641025641024,
      "acc_norm_stderr": 0.024939313906940774
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.27037037037037037,
      "acc_stderr": 0.02708037281514566,
      "acc_norm": 0.27037037037037037,
      "acc_norm_stderr": 0.02708037281514566
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.39915966386554624,
      "acc_stderr": 0.031811100324139245,
      "acc_norm": 0.39915966386554624,
      "acc_norm_stderr": 0.031811100324139245
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33774834437086093,
      "acc_stderr": 0.03861557546255169,
      "acc_norm": 0.33774834437086093,
      "acc_norm_stderr": 0.03861557546255169
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6055045871559633,
      "acc_stderr": 0.020954642108587468,
      "acc_norm": 0.6055045871559633,
      "acc_norm_stderr": 0.020954642108587468
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3287037037037037,
      "acc_stderr": 0.03203614084670058,
      "acc_norm": 0.3287037037037037,
      "acc_norm_stderr": 0.03203614084670058
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.5686274509803921,
      "acc_stderr": 0.03476099060501636,
      "acc_norm": 0.5686274509803921,
      "acc_norm_stderr": 0.03476099060501636
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.5991561181434599,
      "acc_stderr": 0.031900803894732356,
      "acc_norm": 0.5991561181434599,
      "acc_norm_stderr": 0.031900803894732356
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5381165919282511,
      "acc_stderr": 0.033460150119732274,
      "acc_norm": 0.5381165919282511,
      "acc_norm_stderr": 0.033460150119732274
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5190839694656488,
      "acc_stderr": 0.04382094705550988,
      "acc_norm": 0.5190839694656488,
      "acc_norm_stderr": 0.04382094705550988
    },
    "hendrycksTest-international_law": {
      "acc": 0.6528925619834711,
      "acc_stderr": 0.04345724570292534,
      "acc_norm": 0.6528925619834711,
      "acc_norm_stderr": 0.04345724570292534
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.04830366024635331,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5828220858895705,
      "acc_stderr": 0.038741028598180814,
      "acc_norm": 0.5828220858895705,
      "acc_norm_stderr": 0.038741028598180814
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.375,
      "acc_stderr": 0.04595091388086298,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.04595091388086298
    },
    "hendrycksTest-management": {
      "acc": 0.5339805825242718,
      "acc_stderr": 0.0493929144727348,
      "acc_norm": 0.5339805825242718,
      "acc_norm_stderr": 0.0493929144727348
    },
    "hendrycksTest-marketing": {
      "acc": 0.6452991452991453,
      "acc_stderr": 0.03134250486245402,
      "acc_norm": 0.6452991452991453,
      "acc_norm_stderr": 0.03134250486245402
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.58,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6730523627075351,
      "acc_stderr": 0.016774908180131463,
      "acc_norm": 0.6730523627075351,
      "acc_norm_stderr": 0.016774908180131463
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4913294797687861,
      "acc_stderr": 0.026915047355369804,
      "acc_norm": 0.4913294797687861,
      "acc_norm_stderr": 0.026915047355369804
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2860335195530726,
      "acc_stderr": 0.015113972129062136,
      "acc_norm": 0.2860335195530726,
      "acc_norm_stderr": 0.015113972129062136
    },
    "hendrycksTest-nutrition": {
      "acc": 0.45098039215686275,
      "acc_stderr": 0.028491993586171566,
      "acc_norm": 0.45098039215686275,
      "acc_norm_stderr": 0.028491993586171566
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5112540192926045,
      "acc_stderr": 0.028390897396863537,
      "acc_norm": 0.5112540192926045,
      "acc_norm_stderr": 0.028390897396863537
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5216049382716049,
      "acc_stderr": 0.027794760105008736,
      "acc_norm": 0.5216049382716049,
      "acc_norm_stderr": 0.027794760105008736
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3120567375886525,
      "acc_stderr": 0.027640120545169927,
      "acc_norm": 0.3120567375886525,
      "acc_norm_stderr": 0.027640120545169927
    },
    "hendrycksTest-professional_law": {
      "acc": 0.35071707953063885,
      "acc_stderr": 0.012187773370741523,
      "acc_norm": 0.35071707953063885,
      "acc_norm_stderr": 0.012187773370741523
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4852941176470588,
      "acc_stderr": 0.03035969707904612,
      "acc_norm": 0.4852941176470588,
      "acc_norm_stderr": 0.03035969707904612
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4673202614379085,
      "acc_stderr": 0.020184583359102202,
      "acc_norm": 0.4673202614379085,
      "acc_norm_stderr": 0.020184583359102202
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5363636363636364,
      "acc_stderr": 0.04776449162396197,
      "acc_norm": 0.5363636363636364,
      "acc_norm_stderr": 0.04776449162396197
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4897959183673469,
      "acc_stderr": 0.03200255347893782,
      "acc_norm": 0.4897959183673469,
      "acc_norm_stderr": 0.03200255347893782
    },
    "hendrycksTest-sociology": {
      "acc": 0.5373134328358209,
      "acc_stderr": 0.03525675167467974,
      "acc_norm": 0.5373134328358209,
      "acc_norm_stderr": 0.03525675167467974
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-virology": {
      "acc": 0.46987951807228917,
      "acc_stderr": 0.03885425420866767,
      "acc_norm": 0.46987951807228917,
      "acc_norm_stderr": 0.03885425420866767
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6257309941520468,
      "acc_stderr": 0.03711601185389483,
      "acc_norm": 0.6257309941520468,
      "acc_norm_stderr": 0.03711601185389483
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/scratch/project_462000319/general-tools/checkpoints/33B_torch_step238418_bfloat16,use_accelerate=True,tokenizer=/scratch/project_462000319/general-tools/checkpoints/33B_torch_step238418_bfloat16,dtype=bfloat16,trust_remote_code=False",
    "num_fewshot": 5,
    "batch_size": null,
    "batch_sizes": [],
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}