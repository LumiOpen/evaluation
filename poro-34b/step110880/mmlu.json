{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-anatomy": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.03785714465066653,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.03785714465066653
    },
    "hendrycksTest-astronomy": {
      "acc": 0.29605263157894735,
      "acc_stderr": 0.03715062154998905,
      "acc_norm": 0.29605263157894735,
      "acc_norm_stderr": 0.03715062154998905
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.35094339622641507,
      "acc_stderr": 0.029373646253234686,
      "acc_norm": 0.35094339622641507,
      "acc_norm_stderr": 0.029373646253234686
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.039420826399272135,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.039420826399272135
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2947976878612717,
      "acc_stderr": 0.03476599607516478,
      "acc_norm": 0.2947976878612717,
      "acc_norm_stderr": 0.03476599607516478
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237655,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237655
    },
    "hendrycksTest-computer_security": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.28936170212765955,
      "acc_stderr": 0.029644006577009618,
      "acc_norm": 0.28936170212765955,
      "acc_norm_stderr": 0.029644006577009618
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2543859649122807,
      "acc_stderr": 0.040969851398436716,
      "acc_norm": 0.2543859649122807,
      "acc_norm_stderr": 0.040969851398436716
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.38620689655172413,
      "acc_stderr": 0.04057324734419035,
      "acc_norm": 0.38620689655172413,
      "acc_norm_stderr": 0.04057324734419035
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2751322751322751,
      "acc_stderr": 0.023000086859068642,
      "acc_norm": 0.2751322751322751,
      "acc_norm_stderr": 0.023000086859068642
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.03809523809523812,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.03809523809523812
    },
    "hendrycksTest-global_facts": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.2838709677419355,
      "acc_stderr": 0.02564938106302927,
      "acc_norm": 0.2838709677419355,
      "acc_norm_stderr": 0.02564938106302927
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.30049261083743845,
      "acc_stderr": 0.03225799476233483,
      "acc_norm": 0.30049261083743845,
      "acc_norm_stderr": 0.03225799476233483
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.03756335775187897,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03756335775187897
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.2878787878787879,
      "acc_stderr": 0.03225883512300993,
      "acc_norm": 0.2878787878787879,
      "acc_norm_stderr": 0.03225883512300993
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.38860103626943004,
      "acc_stderr": 0.03517739796373133,
      "acc_norm": 0.38860103626943004,
      "acc_norm_stderr": 0.03517739796373133
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.28717948717948716,
      "acc_stderr": 0.02293992541853062,
      "acc_norm": 0.28717948717948716,
      "acc_norm_stderr": 0.02293992541853062
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.23333333333333334,
      "acc_stderr": 0.02578787422095931,
      "acc_norm": 0.23333333333333334,
      "acc_norm_stderr": 0.02578787422095931
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3025210084033613,
      "acc_stderr": 0.02983796238829193,
      "acc_norm": 0.3025210084033613,
      "acc_norm_stderr": 0.02983796238829193
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.271523178807947,
      "acc_stderr": 0.036313298039696525,
      "acc_norm": 0.271523178807947,
      "acc_norm_stderr": 0.036313298039696525
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3394495412844037,
      "acc_stderr": 0.020302109342662345,
      "acc_norm": 0.3394495412844037,
      "acc_norm_stderr": 0.020302109342662345
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.03350991604696043,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.03350991604696043
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3284313725490196,
      "acc_stderr": 0.03296245110172229,
      "acc_norm": 0.3284313725490196,
      "acc_norm_stderr": 0.03296245110172229
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03068582059661081,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03068582059661081
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3183856502242152,
      "acc_stderr": 0.03126580522513713,
      "acc_norm": 0.3183856502242152,
      "acc_norm_stderr": 0.03126580522513713
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.26717557251908397,
      "acc_stderr": 0.03880848301082395,
      "acc_norm": 0.26717557251908397,
      "acc_norm_stderr": 0.03880848301082395
    },
    "hendrycksTest-international_law": {
      "acc": 0.33884297520661155,
      "acc_stderr": 0.04320767807536669,
      "acc_norm": 0.33884297520661155,
      "acc_norm_stderr": 0.04320767807536669
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3425925925925926,
      "acc_stderr": 0.045879047413018105,
      "acc_norm": 0.3425925925925926,
      "acc_norm_stderr": 0.045879047413018105
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.27607361963190186,
      "acc_stderr": 0.03512385283705051,
      "acc_norm": 0.27607361963190186,
      "acc_norm_stderr": 0.03512385283705051
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25892857142857145,
      "acc_stderr": 0.04157751539865629,
      "acc_norm": 0.25892857142857145,
      "acc_norm_stderr": 0.04157751539865629
    },
    "hendrycksTest-management": {
      "acc": 0.34951456310679613,
      "acc_stderr": 0.047211885060971716,
      "acc_norm": 0.34951456310679613,
      "acc_norm_stderr": 0.047211885060971716
    },
    "hendrycksTest-marketing": {
      "acc": 0.3547008547008547,
      "acc_stderr": 0.03134250486245402,
      "acc_norm": 0.3547008547008547,
      "acc_norm_stderr": 0.03134250486245402
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.01726860756000577,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.01726860756000577
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3179190751445087,
      "acc_stderr": 0.02507071371915319,
      "acc_norm": 0.3179190751445087,
      "acc_norm_stderr": 0.02507071371915319
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.27150837988826815,
      "acc_stderr": 0.014874252168095268,
      "acc_norm": 0.27150837988826815,
      "acc_norm_stderr": 0.014874252168095268
    },
    "hendrycksTest-nutrition": {
      "acc": 0.3137254901960784,
      "acc_stderr": 0.02656892101545715,
      "acc_norm": 0.3137254901960784,
      "acc_norm_stderr": 0.02656892101545715
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3408360128617363,
      "acc_stderr": 0.02692084126077616,
      "acc_norm": 0.3408360128617363,
      "acc_norm_stderr": 0.02692084126077616
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3487654320987654,
      "acc_stderr": 0.026517597724465013,
      "acc_norm": 0.3487654320987654,
      "acc_norm_stderr": 0.026517597724465013
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.026684564340460987,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.026684564340460987
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2900912646675359,
      "acc_stderr": 0.011590375554733098,
      "acc_norm": 0.2900912646675359,
      "acc_norm_stderr": 0.011590375554733098
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.41544117647058826,
      "acc_stderr": 0.029935342707877753,
      "acc_norm": 0.41544117647058826,
      "acc_norm_stderr": 0.029935342707877753
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.32679738562091504,
      "acc_stderr": 0.018975427920507208,
      "acc_norm": 0.32679738562091504,
      "acc_norm_stderr": 0.018975427920507208
    },
    "hendrycksTest-public_relations": {
      "acc": 0.37272727272727274,
      "acc_stderr": 0.04631381319425463,
      "acc_norm": 0.37272727272727274,
      "acc_norm_stderr": 0.04631381319425463
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3469387755102041,
      "acc_stderr": 0.030472526026726496,
      "acc_norm": 0.3469387755102041,
      "acc_norm_stderr": 0.030472526026726496
    },
    "hendrycksTest-sociology": {
      "acc": 0.27860696517412936,
      "acc_stderr": 0.031700561834973086,
      "acc_norm": 0.27860696517412936,
      "acc_norm_stderr": 0.031700561834973086
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-virology": {
      "acc": 0.23493975903614459,
      "acc_stderr": 0.03300533186128922,
      "acc_norm": 0.23493975903614459,
      "acc_norm_stderr": 0.03300533186128922
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.03565079670708311,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.03565079670708311
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/scratch/project_462000319/general-tools/checkpoints/33B_torch_step110880_bfloat16,use_accelerate=True,tokenizer=/scratch/project_462000319/general-tools/checkpoints/33B_torch_step110880_bfloat16,dtype=bfloat16,trust_remote_code=False",
    "num_fewshot": 5,
    "batch_size": null,
    "batch_sizes": [],
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}