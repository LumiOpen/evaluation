{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.039725528847851375
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.03782728980865469,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.03782728980865469
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.2981132075471698,
      "acc_stderr": 0.028152837942493864,
      "acc_norm": 0.2981132075471698,
      "acc_norm_stderr": 0.028152837942493864
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.037455547914624555,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.037455547914624555
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.22,
      "acc_stderr": 0.04163331998932269,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3236994219653179,
      "acc_stderr": 0.0356760379963917,
      "acc_norm": 0.3236994219653179,
      "acc_norm_stderr": 0.0356760379963917
    },
    "hendrycksTest-college_physics": {
      "acc": 0.13725490196078433,
      "acc_stderr": 0.034240846698915216,
      "acc_norm": 0.13725490196078433,
      "acc_norm_stderr": 0.034240846698915216
    },
    "hendrycksTest-computer_security": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001974,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001974
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3021276595744681,
      "acc_stderr": 0.030017554471880557,
      "acc_norm": 0.3021276595744681,
      "acc_norm_stderr": 0.030017554471880557
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.04227054451232201,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.04227054451232201
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3310344827586207,
      "acc_stderr": 0.03921545312467122,
      "acc_norm": 0.3310344827586207,
      "acc_norm_stderr": 0.03921545312467122
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.02264421261552521,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.02264421261552521
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.21428571428571427,
      "acc_stderr": 0.03670066451047181,
      "acc_norm": 0.21428571428571427,
      "acc_norm_stderr": 0.03670066451047181
    },
    "hendrycksTest-global_facts": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.25806451612903225,
      "acc_stderr": 0.02489246917246284,
      "acc_norm": 0.25806451612903225,
      "acc_norm_stderr": 0.02489246917246284
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.20689655172413793,
      "acc_stderr": 0.02850137816789395,
      "acc_norm": 0.20689655172413793,
      "acc_norm_stderr": 0.02850137816789395
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.04793724854411019,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.04793724854411019
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3575757575757576,
      "acc_stderr": 0.03742597043806586,
      "acc_norm": 0.3575757575757576,
      "acc_norm_stderr": 0.03742597043806586
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.03191178226713548,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.03191178226713548
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.38860103626943004,
      "acc_stderr": 0.03517739796373133,
      "acc_norm": 0.38860103626943004,
      "acc_norm_stderr": 0.03517739796373133
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.24102564102564103,
      "acc_stderr": 0.021685546665333198,
      "acc_norm": 0.24102564102564103,
      "acc_norm_stderr": 0.021685546665333198
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.22962962962962963,
      "acc_stderr": 0.02564410863926764,
      "acc_norm": 0.22962962962962963,
      "acc_norm_stderr": 0.02564410863926764
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.27310924369747897,
      "acc_stderr": 0.028942004040998174,
      "acc_norm": 0.27310924369747897,
      "acc_norm_stderr": 0.028942004040998174
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.271523178807947,
      "acc_stderr": 0.03631329803969653,
      "acc_norm": 0.271523178807947,
      "acc_norm_stderr": 0.03631329803969653
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.27706422018348625,
      "acc_stderr": 0.019188482590169545,
      "acc_norm": 0.27706422018348625,
      "acc_norm_stderr": 0.019188482590169545
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.22685185185185186,
      "acc_stderr": 0.02856165010242226,
      "acc_norm": 0.22685185185185186,
      "acc_norm_stderr": 0.02856165010242226
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.034411900234824655,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.034411900234824655
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03068582059661081,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03068582059661081
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3901345291479821,
      "acc_stderr": 0.03273766725459156,
      "acc_norm": 0.3901345291479821,
      "acc_norm_stderr": 0.03273766725459156
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.2824427480916031,
      "acc_stderr": 0.03948406125768361,
      "acc_norm": 0.2824427480916031,
      "acc_norm_stderr": 0.03948406125768361
    },
    "hendrycksTest-international_law": {
      "acc": 0.371900826446281,
      "acc_stderr": 0.04412015806624504,
      "acc_norm": 0.371900826446281,
      "acc_norm_stderr": 0.04412015806624504
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199986,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199986
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.26993865030674846,
      "acc_stderr": 0.03487825168497892,
      "acc_norm": 0.26993865030674846,
      "acc_norm_stderr": 0.03487825168497892
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.3125,
      "acc_stderr": 0.043994650575715215,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.043994650575715215
    },
    "hendrycksTest-management": {
      "acc": 0.32038834951456313,
      "acc_stderr": 0.0462028408228004,
      "acc_norm": 0.32038834951456313,
      "acc_norm_stderr": 0.0462028408228004
    },
    "hendrycksTest-marketing": {
      "acc": 0.3717948717948718,
      "acc_stderr": 0.031660988918880785,
      "acc_norm": 0.3717948717948718,
      "acc_norm_stderr": 0.031660988918880785
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.4061302681992337,
      "acc_stderr": 0.017562037406478916,
      "acc_norm": 0.4061302681992337,
      "acc_norm_stderr": 0.017562037406478916
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3063583815028902,
      "acc_stderr": 0.024818350129436593,
      "acc_norm": 0.3063583815028902,
      "acc_norm_stderr": 0.024818350129436593
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2446927374301676,
      "acc_stderr": 0.014378169884098433,
      "acc_norm": 0.2446927374301676,
      "acc_norm_stderr": 0.014378169884098433
    },
    "hendrycksTest-nutrition": {
      "acc": 0.28104575163398693,
      "acc_stderr": 0.02573885479781873,
      "acc_norm": 0.28104575163398693,
      "acc_norm_stderr": 0.02573885479781873
    },
    "hendrycksTest-philosophy": {
      "acc": 0.36012861736334406,
      "acc_stderr": 0.027264297599804015,
      "acc_norm": 0.36012861736334406,
      "acc_norm_stderr": 0.027264297599804015
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.025842248700902168,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.025842248700902168
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.26595744680851063,
      "acc_stderr": 0.026358065698880585,
      "acc_norm": 0.26595744680851063,
      "acc_norm_stderr": 0.026358065698880585
    },
    "hendrycksTest-professional_law": {
      "acc": 0.30182529335071706,
      "acc_stderr": 0.01172435051810589,
      "acc_norm": 0.30182529335071706,
      "acc_norm_stderr": 0.01172435051810589
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4007352941176471,
      "acc_stderr": 0.02976826352893311,
      "acc_norm": 0.4007352941176471,
      "acc_norm_stderr": 0.02976826352893311
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.31699346405228757,
      "acc_stderr": 0.018824219512706214,
      "acc_norm": 0.31699346405228757,
      "acc_norm_stderr": 0.018824219512706214
    },
    "hendrycksTest-public_relations": {
      "acc": 0.37272727272727274,
      "acc_stderr": 0.046313813194254635,
      "acc_norm": 0.37272727272727274,
      "acc_norm_stderr": 0.046313813194254635
    },
    "hendrycksTest-security_studies": {
      "acc": 0.27755102040816326,
      "acc_stderr": 0.028666857790274648,
      "acc_norm": 0.27755102040816326,
      "acc_norm_stderr": 0.028666857790274648
    },
    "hendrycksTest-sociology": {
      "acc": 0.3283582089552239,
      "acc_stderr": 0.033206858897443244,
      "acc_norm": 0.3283582089552239,
      "acc_norm_stderr": 0.033206858897443244
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.47,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-virology": {
      "acc": 0.26506024096385544,
      "acc_stderr": 0.03436024037944967,
      "acc_norm": 0.26506024096385544,
      "acc_norm_stderr": 0.03436024037944967
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4327485380116959,
      "acc_stderr": 0.03799978644370606,
      "acc_norm": 0.4327485380116959,
      "acc_norm_stderr": 0.03799978644370606
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/scratch/project_462000319/general-tools/checkpoints/33B_torch_step119232_bfloat16,use_accelerate=True,tokenizer=/scratch/project_462000319/tokenizers/tokenizer_v6_fixed_fin,dtype=bfloat16,trust_remote_code=False",
    "num_fewshot": 5,
    "batch_size": null,
    "batch_sizes": [],
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}